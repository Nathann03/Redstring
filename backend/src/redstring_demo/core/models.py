"""Core dataclasses shared across the hybrid retrieval pipeline."""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional


@dataclass(frozen=True)
class PlayerQuery:
    """Payload received from the game client for an NPC interaction."""

    save_id: str
    player_id: str
    npc_id: str
    player_question: str
    game_state: Dict[str, Any]


@dataclass
class WarmupStatus:
    """Status returned when the LLM is not yet ready."""

    is_llm_ready: bool
    preset_dialogue: Optional[str]
    show_spinner: bool
    queued: bool
    queue_length: int = 0


class RetrievalHitType(str, Enum):
    """Classification of retrieval outcomes."""

    MISS = "miss"
    FUZZY = "fuzzy"
    EXACT = "exact"


@dataclass
class RetrievalResult:
    """Result of embedding-based retrieval of pregenerated responses."""

    hit_type: RetrievalHitType
    similarity: float
    response_text: Optional[str] = None
    source_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class RAGContext:
    """Aggregated context fed into the LLM fallback."""

    player_question: str
    dynamic_facts: Dict[str, Any]
    anchor_facts: List[str]
    retrieved_snippets: List[str]


@dataclass
class LLMResponse:
    """Text generated by the local LLM."""

    text: str
    reasoning: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AudioStream:
    """Metadata returned by the Turbo TTS stub."""

    text: str
    latency_ms: int
    sample_rate_hz: int
    format: str = "pcm16"


@dataclass
class PipelineOutput:
    """End-to-end output for the orchestrator."""

    text: str
    source: str
    warmup: Optional[WarmupStatus] = None
    audio: Optional[AudioStream] = None
    similarity: Optional[float] = None
    cached: bool = False
    spinner_message: Optional[str] = None

